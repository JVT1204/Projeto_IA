{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JVT1204/Projeto_IA/blob/main/Projeto_N2_NoticiasEsporte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40b90843"
      },
      "source": [
        "<img src=\"http://meusite.mackenzie.br/rogerio/mackenzie_logo/UPM.2_horizontal_vermelho.jpg\"  width=300, align=\"right\">\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Colab do Projeto Semestral - Classificador de notícias de esporte com IA**\n",
        "---\n",
        "\n",
        "Atenção, podem ser que nem todas as tarefas sejam executadas no Colab (a aplicação por exemplo, pode estar hospedada no streamlit cloud). Mas a maior parte pode estar aqui ou ao menos indicada e comentada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR6kcPlTeV_n"
      },
      "source": [
        "Além disso a entrega deve incluir:\n",
        "\n",
        "1. **Um GitHub público do projeto**\n",
        "2. **Código completo e executável em um notebook Python (este template)**\n",
        "3. **Uma aplicação streamlit para consumo do modelo**\n",
        "4. **Um texto/artigo do projeto**\n",
        "5. **Um vídeo (link YouTube ou outro) de no máximo 3min de apresentação do projeto**\n",
        "\n",
        "Um **`readme.md`** no GitHub público do projeto deve indicar (um índice) cada uma dessas entregas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rYx9D4GZA5o9"
      },
      "outputs": [],
      "source": [
        "#@title **Identificação do Grupo**\n",
        "\n",
        "#@markdown Integrantes do Grupo, nome completo em orgem alfabética (*informe \\<RA\\>,\\<nome\\>*)\n",
        "Aluno1 = '10223349, Daniel Reis Raske' #@param {type:\"string\"}\n",
        "Aluno2 = '10400734, Eduardo Marui de Camargo' #@param {type:\"string\"}\n",
        "Aluno3 = '10403378, Victor Vergara Marques de Oliveira' #@param {type:\"string\"}\n",
        "Aluno4 = '10204809, Vitor dos Santos Souza' #@param {type:\"string\"}\n",
        "Aluno5 = '10402674, João Vitor Tortorello' #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-MbC50IHTmh3"
      },
      "outputs": [],
      "source": [
        "#@title Assinale aqui a sua opção de Projeto\n",
        "Projeto = \"IA Aplicada a Documentos: Uso de Grandes Modelos de Linguagem Abertos\" #@param [\"IA Aplicada a Imagens: Uso de Modelos de Redes Neurais\", \"IA Aplicada a Documentos: Uso de Grandes Modelos de Linguagem Abertos\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxYbSf6mVM7y"
      },
      "source": [
        "# **Resumo**\n",
        "\n",
        "## 1. Objetivo do Projeto\n",
        "\n",
        "Este projeto tem como objetivo desenvolver um sistema de classificação automática de notícias de futebol em português, utilizando técnicas de Processamento de Linguagem Natural (NLP) e Aprendizado de Máquina. O sistema é capaz de categorizar as notícias em quatro categorias principais:\n",
        "\n",
        "- **Resultados**: Notícias sobre jogos, placares e resultados de partidas\n",
        "- **Transferências**: Notícias sobre negociações, contratações e vendas de jogadores\n",
        "- **Lesões**: Notícias sobre condições físicas, recuperações e ausências de jogadores\n",
        "- **Táticas**: Notícias sobre estratégias, formações e preparação das equipes\n",
        "\n",
        "## 2. Fontes dos Dados\n",
        "\n",
        "Os dados são coletados automaticamente de três principais portais de notícias esportivas brasileiros:\n",
        "\n",
        "- GE (Globo Esporte)\n",
        "- UOL Esporte\n",
        "- Terra Esportes\n",
        "\n",
        "O coletor de notícias é desenvolvido para extrair automaticamente títulos, resumos e URLs das notícias mais recentes de cada fonte, garantindo uma base de dados diversificada e atualizada.\n",
        "\n",
        "## 3. Ferramentas e Tecnologias\n",
        "\n",
        "Para o desenvolvimento do projeto, são utilizadas as seguintes tecnologias:\n",
        "\n",
        "- **Processamento de Texto**:\n",
        "  - spaCy (modelo pt_core_news_lg) para processamento de linguagem natural em português\n",
        "  - NLTK para pré-processamento de texto\n",
        "\n",
        "- **Modelo de Classificação**:\n",
        "  - BERT (Bidirectional Encoder Representations from Transformers)\n",
        "  - Modelo base: neuralmind/bert-base-portuguese-cased\n",
        "  - Fine-tuning para classificação de notícias\n",
        "\n",
        "- **Frameworks e Bibliotecas**:\n",
        "  - PyTorch para implementação do modelo\n",
        "  - Transformers (Hugging Face) para acesso ao modelo BERT\n",
        "  - Pandas e NumPy para manipulação de dados\n",
        "  - Scikit-learn para avaliação do modelo\n",
        "  - Streamlit para interface de usuário\n",
        "\n",
        "## 4. Resultados Preliminares\n",
        "\n",
        "O sistema desenvolvido apresenta as seguintes características:\n",
        "\n",
        "- **Coleta Automática**: Sistema capaz de coletar notícias em tempo real das principais fontes\n",
        "- **Pré-processamento Robusto**: Tratamento adequado de textos em português, incluindo remoção de stopwords e lematização\n",
        "- **Classificação Multiclasse**: Modelo capaz de categorizar notícias em quatro categorias distintas\n",
        "- **Interface Web**: Aplicação Streamlit para fácil consumo do modelo\n",
        "\n",
        "O modelo é treinado com um conjunto de dados balanceado e avaliado usando métricas de classificação multiclasse, demonstrando boa capacidade de generalização e precisão na categorização das notícias.\n",
        "\n",
        "A implementação final permite não apenas a classificação automática de notícias, mas também oferece insights sobre a distribuição de categorias ao longo do tempo, facilitando a análise de tendências no jornalismo esportivo brasileiro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctroSu6jNABS"
      },
      "source": [
        "# **Apresentação dos dados**\n",
        "\n",
        "## Exemplos de Notícias Coletadas\n",
        "\n",
        "Para demonstrar o funcionamento do coletor de notícias, vamos mostrar alguns exemplos de notícias coletadas de cada fonte:\n",
        "\n",
        "```python\n",
        "# Exemplo de código para visualizar as notícias coletadas\n",
        "import pandas as pd\n",
        "\n",
        "# Carrega os dados coletados\n",
        "df = pd.read_csv('data/raw/collected_news.csv')\n",
        "\n",
        "# Mostra exemplos de cada fonte\n",
        "print(\"Exemplos de notícias do GE:\")\n",
        "print(df[df['source'] == 'ge'][['title', 'summary']].head(2))\n",
        "print(\"\\nExemplos de notícias do UOL:\")\n",
        "print(df[df['source'] == 'uol'][['title', 'summary']].head(2))\n",
        "print(\"\\nExemplos de notícias do Terra:\")\n",
        "print(df[df['source'] == 'terra'][['title', 'summary']].head(2))\n",
        "```\n",
        "\n",
        "## Estatísticas Básicas da Coleta\n",
        "\n",
        "```python\n",
        "# Análise da distribuição por fonte\n",
        "print(\"Distribuição de notícias por fonte:\")\n",
        "print(df['source'].value_counts())\n",
        "\n",
        "# Análise do tamanho dos textos\n",
        "df['text_length'] = df['title'].str.len() + df['summary'].str.len()\n",
        "print(\"\\nEstatísticas do tamanho dos textos:\")\n",
        "print(df['text_length'].describe())\n",
        "```\n",
        "\n",
        "## Visualização da Distribuição\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configuração do estilo\n",
        "plt.style.use('seaborn')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "# Gráfico de barras da distribuição por fonte\n",
        "plt.figure(figsize=(10, 6))\n",
        "df['source'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribuição de Notícias por Fonte')\n",
        "plt.xlabel('Fonte')\n",
        "plt.ylabel('Quantidade de Notícias')\n",
        "plt.show()\n",
        "\n",
        "# Histograma do tamanho dos textos\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=df, x='text_length', bins=30)\n",
        "plt.title('Distribuição do Tamanho dos Textos')\n",
        "plt.xlabel('Tamanho do Texto (caracteres)')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Características dos Dados\n",
        "\n",
        "1. **Fontes Diversificadas**:\n",
        "   - GE (Globo Esporte)\n",
        "   - UOL Esporte\n",
        "   - Terra Esportes\n",
        "\n",
        "2. **Estrutura dos Dados**:\n",
        "   - Título da notícia\n",
        "   - Resumo/descrição\n",
        "   - URL da notícia\n",
        "   - Fonte\n",
        "   - Data de coleta\n",
        "\n",
        "3. **Volume de Dados**:\n",
        "   - Coleta automática de até 50 notícias por fonte\n",
        "   - Atualização diária\n",
        "   - Base de dados em constante crescimento\n",
        "\n",
        "4. **Qualidade dos Dados**:\n",
        "   - Textos em português\n",
        "   - Formato padronizado\n",
        "   - Conteúdo relevante para classificação\n",
        "\n",
        "## Link para o Dataset\n",
        "\n",
        "O dataset completo está disponível em: https://github.com/JVT1204/Projeto_IA.git\n",
        "\n",
        "## Próximos Passos\n",
        "\n",
        "Após a coleta inicial, os dados passam por:\n",
        "1. Pré-processamento de texto\n",
        "2. Categorização automática\n",
        "3. Preparação para treinamento do modelo\n",
        "\n",
        "Os resultados dessas etapas serão apresentados nas seções seguintes do notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUPW64ESNJgR"
      },
      "source": [
        "# ***Instalação das Dependências***\n",
        "\n",
        "!pip install pandas scikit-learn numpy matplotlib seaborn torch transformers beautifulsoup4 requests spacy\n",
        "\n",
        "!python -m spacy download pt_core_news_lg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDzwn_5AMZ52"
      },
      "source": [
        "# **Importação das bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import spacy\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import unicodedata\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "rpgH8rThJidD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuração de logging**"
      ],
      "metadata": {
        "id": "hZjKEFgOJOyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "LKTXbCijI2uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuração de visualização**"
      ],
      "metadata": {
        "id": "O2DFw2QqJ5Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn')\n",
        "sns.set_palette('husl')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "UQ36ehTSJ9B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coletor de Notícias**"
      ],
      "metadata": {
        "id": "aFWF8ml1KOeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsCollector:\n",
        "    def __init__(self):\n",
        "        self.sources = {\n",
        "            'ge': 'https://ge.globo.com/futebol/',\n",
        "            'uol': 'https://www.uol.com.br/esporte/futebol/',\n",
        "            'terra': 'https://www.terra.com.br/esportes/futebol/'\n",
        "        }\n",
        "\n",
        "    def collect_news(self, max_news_per_source=50):\n",
        "        all_news = []\n",
        "\n",
        "        for source, url in self.sources.items():\n",
        "            try:\n",
        "                logger.info(f\"Coletando notícias de {source}...\")\n",
        "                response = requests.get(url)\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Coleta notícias (implementação específica para cada fonte)\n",
        "                if source == 'ge':\n",
        "                    news = self._collect_ge(soup, max_news_per_source)\n",
        "                elif source == 'uol':\n",
        "                    news = self._collect_uol(soup, max_news_per_source)\n",
        "                elif source == 'terra':\n",
        "                    news = self._collect_terra(soup, max_news_per_source)\n",
        "\n",
        "                all_news.extend(news)\n",
        "                logger.info(f\"Coletadas {len(news)} notícias de {source}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro ao coletar notícias de {source}: {str(e)}\")\n",
        "\n",
        "        # Adiciona timestamp de coleta\n",
        "        for news in all_news:\n",
        "            news['collected_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "        return pd.DataFrame(all_news)\n",
        "\n",
        "    def _collect_ge(self, soup, max_news):\n",
        "        news = []\n",
        "        articles = soup.find_all('div', class_='feed-post-body')[:max_news]\n",
        "\n",
        "        for article in articles:\n",
        "            try:\n",
        "                title = article.find('a', class_='feed-post-link').text.strip()\n",
        "                summary = article.find('div', class_='feed-post-body-resumo')\n",
        "                summary = summary.text.strip() if summary else \"\"\n",
        "                url = article.find('a', class_='feed-post-link')['href']\n",
        "\n",
        "                news.append({\n",
        "                    'title': title,\n",
        "                    'summary': summary,\n",
        "                    'url': url,\n",
        "                    'source': 'ge'\n",
        "                })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro ao processar notícia do GE: {str(e)}\")\n",
        "\n",
        "        return news\n",
        "\n",
        "    def _collect_uol(self, soup, max_news):\n",
        "        news = []\n",
        "        articles = soup.find_all('div', class_='thumbnails-wrapper')[:max_news]\n",
        "\n",
        "        for article in articles:\n",
        "            try:\n",
        "                title = article.find('h3', class_='title').text.strip()\n",
        "                summary = article.find('p', class_='description')\n",
        "                summary = summary.text.strip() if summary else \"\"\n",
        "                url = article.find('a')['href']\n",
        "\n",
        "                news.append({\n",
        "                    'title': title,\n",
        "                    'summary': summary,\n",
        "                    'url': url,\n",
        "                    'source': 'uol'\n",
        "                })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro ao processar notícia do UOL: {str(e)}\")\n",
        "\n",
        "        return news\n",
        "\n",
        "    def _collect_terra(self, soup, max_news):\n",
        "        news = []\n",
        "        articles = soup.find_all('div', class_='card-news')[:max_news]\n",
        "\n",
        "        for article in articles:\n",
        "            try:\n",
        "                title = article.find('h3', class_='card-news__title').text.strip()\n",
        "                summary = article.find('p', class_='card-news__description')\n",
        "                summary = summary.text.strip() if summary else \"\"\n",
        "                url = article.find('a')['href']\n",
        "\n",
        "                news.append({\n",
        "                    'title': title,\n",
        "                    'summary': summary,\n",
        "                    'url': url,\n",
        "                    'source': 'terra'\n",
        "                })\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Erro ao processar notícia do Terra: {str(e)}\")\n",
        "\n",
        "        return news\n",
        "\n",
        "    def save_news(self, df, output_dir='data/raw'):\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "        output_file = Path(output_dir) / 'collected_news.csv'\n",
        "        df.to_csv(output_file, index=False)\n",
        "        logger.info(f\"Notícias salvas em: {output_file}\")"
      ],
      "metadata": {
        "id": "EbY7-r4SKSLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teste do coletor**"
      ],
      "metadata": {
        "id": "gx5SJ9ZvKkzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collector = NewsCollector()\n",
        "news_df = collector.collect_news(max_news_per_source=10)  # Coleta 10 notícias de cada fonte para teste\n",
        "collector.save_news(news_df)"
      ],
      "metadata": {
        "id": "-QI4_Z4sKp39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processamento de Texto**"
      ],
      "metadata": {
        "id": "F_Y9xsRxP1TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Pré-processa o texto para análise.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Carrega o modelo do spaCy\n",
        "    nlp = spacy.load('pt_core_news_lg')\n",
        "\n",
        "    # Normaliza o texto\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove caracteres especiais\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    # Remove espaços extras\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Processa com spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Remove stopwords e lematiza\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.category_keywords = {\n",
        "            \"resultado\": [\n",
        "                \"vitória\", \"derrota\", \"empate\", \"gols\", \"placar\", \"jogo\", \"partida\",\n",
        "                \"campeonato\", \"torneio\", \"competição\", \"estádio\", \"time\", \"equipe\"\n",
        "            ],\n",
        "            \"transferencia\": [\n",
        "                \"contrato\", \"contratação\", \"venda\", \"compra\", \"negociação\", \"mercado\",\n",
        "                \"clube\", \"time\", \"equipe\", \"jogador\", \"atleta\", \"técnico\", \"treinador\"\n",
        "            ],\n",
        "            \"lesao\": [\n",
        "                \"lesão\", \"machucado\", \"recuperação\", \"tratamento\", \"médico\", \"exame\",\n",
        "                \"cirurgia\", \"fisioterapia\", \"reabilitação\", \"ausência\", \"retorno\"\n",
        "            ],\n",
        "            \"tatica\": [\n",
        "                \"esquema\", \"tático\", \"tática\", \"formação\", \"estratégia\", \"sistema\",\n",
        "                \"treino\", \"preparação\", \"análise\", \"técnico\", \"treinador\", \"jogada\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def process_and_save(self, input_dir='data/raw', output_dir='data/processed'):\n",
        "        try:\n",
        "            # Carrega os dados\n",
        "            df = pd.read_csv(Path(input_dir) / 'collected_news.csv')\n",
        "\n",
        "            # Pré-processa as notícias\n",
        "            df['text'] = df['title'] + \" \" + df['summary'].fillna(\"\")\n",
        "            df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "            df = df[df['processed_text'].str.strip().str.len() > 0]\n",
        "\n",
        "            # Atribui categorias\n",
        "            df['category'] = df['processed_text'].apply(self._get_category)\n",
        "\n",
        "            # Prepara dados para treinamento\n",
        "            category_to_id = {\n",
        "                \"resultado\": 0,\n",
        "                \"transferencia\": 1,\n",
        "                \"lesao\": 2,\n",
        "                \"tatica\": 3,\n",
        "                \"outras\": 4\n",
        "            }\n",
        "\n",
        "            df['label'] = df['category'].map(category_to_id)\n",
        "            training_df = df[['processed_text', 'label']].rename(columns={'processed_text': 'text'})\n",
        "\n",
        "            # Salva os dados\n",
        "            output_path = Path(output_dir)\n",
        "            output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            df.to_csv(output_path / \"processed_news.csv\", index=False)\n",
        "            training_df.to_csv(output_path / \"training_data.csv\", index=False)\n",
        "\n",
        "            # Salva o mapeamento de categorias\n",
        "            mapping = {\n",
        "                'category_to_id': category_to_id,\n",
        "                'id_to_category': {v: k for k, v in category_to_id.items()}\n",
        "            }\n",
        "\n",
        "            with open(output_path / \"category_mapping.json\", \"w\") as f:\n",
        "                json.dump(mapping, f, indent=4)\n",
        "\n",
        "            logger.info(f\"Dados processados salvos em: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Erro no processamento dos dados: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _get_category(self, text):\n",
        "        text = text.lower()\n",
        "        scores = {category: 0 for category in self.category_keywords.keys()}\n",
        "\n",
        "        for category, keywords in self.category_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                if keyword in text:\n",
        "                    scores[category] += 1\n",
        "\n",
        "        if max(scores.values()) == 0:\n",
        "            return \"outras\"\n",
        "\n",
        "        return max(scores.items(), key=lambda x: x[1])[0]"
      ],
      "metadata": {
        "id": "nZd7Jq1BP4m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Teste do processador**"
      ],
      "metadata": {
        "id": "5cFlA9z-QSrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processor = DataProcessor()\n",
        "processor.process_and_save()"
      ],
      "metadata": {
        "id": "MXFYwN1UQW2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Treinamento do Modelo**"
      ],
      "metadata": {
        "id": "d49W5Q25QjGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Fine-Tuning\n",
        "class NewsTrainer:\n",
        "    def __init__(self, num_epochs=5, batch_size=16):\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            'neuralmind/bert-base-portuguese-cased',\n",
        "            num_labels=5\n",
        "        ).to(self.device)\n",
        "\n",
        "    def prepare_data(self, df):\n",
        "        # Divide os dados\n",
        "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "            df['text'].values,\n",
        "            df['label'].values,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Cria os datasets\n",
        "        train_dataset = NewsDataset(train_texts, train_labels, self.tokenizer)\n",
        "        val_dataset = NewsDataset(val_texts, val_labels, self.tokenizer)\n",
        "\n",
        "        # Cria os dataloaders\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.batch_size\n",
        "        )\n",
        "\n",
        "        return train_dataloader, val_dataloader\n",
        "\n",
        "    def train(self, train_dataloader, val_dataloader):\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            # Treinamento\n",
        "            self.model.train()\n",
        "            total_train_loss = 0\n",
        "\n",
        "            for batch in train_dataloader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "            # Validação\n",
        "            self.model.eval()\n",
        "            total_val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_dataloader:\n",
        "                    input_ids = batch['input_ids'].to(self.device)\n",
        "                    attention_mask = batch['attention_mask'].to(self.device)\n",
        "                    labels = batch['labels'].to(self.device)\n",
        "\n",
        "                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "\n",
        "            logger.info(f\"Época {epoch + 1}/{self.num_epochs}\")\n",
        "            logger.info(f\"Loss de treinamento: {avg_train_loss:.4f}\")\n",
        "            logger.info(f\"Loss de validação: {avg_val_loss:.4f}\")\n",
        "\n",
        "            # Salva o melhor modelo\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                self.save_model()\n",
        "\n",
        "    def save_model(self, output_dir='models/best_model'):\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.model.save_pretrained(output_path)\n",
        "        self.tokenizer.save_pretrained(output_path)\n",
        "\n",
        "        logger.info(f\"Modelo salvo em: {output_path}\")"
      ],
      "metadata": {
        "id": "CXgqYKHrQkPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carrega os dados processados e inicia o treinamento do modelo**"
      ],
      "metadata": {
        "id": "JEJGxUhPRHpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/processed/training_data.csv')\n",
        "\n",
        "trainer = NewsTrainer(num_epochs=5, batch_size=16)\n",
        "train_dataloader, val_dataloader = trainer.prepare_data(df)\n",
        "trainer.train(train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "qMQY1QFkRSBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1Evo4PmNhBY"
      },
      "source": [
        "# **Avaliação do modelo**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, tokenizer, test_texts, test_labels):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for text, label in zip(test_texts, test_labels):\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
        "            predictions.append(pred)\n",
        "            true_labels.append(label)\n",
        "\n",
        "    # Calcula métricas\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    import seaborn as sns\n",
        "\n",
        "    # Carrega o mapeamento de categorias\n",
        "    with open('data/processed/category_mapping.json', 'r') as f:\n",
        "        mapping = json.load(f)\n",
        "    id_to_category = mapping['id_to_category']\n",
        "\n",
        "    # Converte IDs para nomes de categorias\n",
        "    predictions = [id_to_category[str(pred)] for pred in predictions]\n",
        "    true_labels = [id_to_category[str(label)] for label in true_labels]\n",
        "\n",
        "    # Gera relatório de classificação\n",
        "    print(\"Relatório de Classificação:\")\n",
        "    print(classification_report(true_labels, predictions))\n",
        "\n",
        "    # Plota matriz de confusão\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=list(set(true_labels)),\n",
        "                yticklabels=list(set(true_labels)))\n",
        "    plt.title('Matriz de Confusão')\n",
        "    plt.xlabel('Predição')\n",
        "    plt.ylabel('Valor Real')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "z8ZrY1ebSeQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carrega o modelo treinado**"
      ],
      "metadata": {
        "id": "5hbQoJmuSvq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('models/best_model')\n",
        "tokenizer = AutoTokenizer.from_pretrained('models/best_model')"
      ],
      "metadata": {
        "id": "18CcKT2RSynC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViQfwNxkNj0C"
      },
      "source": [
        "# **Consumo do modelo**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = df['text'].values[-100:]  # Últimas 100 notícias para teste\n",
        "test_labels = df['label'].values[-100:]\n",
        "evaluate_model(model, tokenizer, test_texts, test_labels)"
      ],
      "metadata": {
        "id": "YQuqXW59TXfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LtXrRFr4hg3"
      },
      "source": [
        "# **Referências**\n",
        "\n",
        "Este é um item obrigatório. Inclua aqui o as referências, fontes, ou bibliografia e sites/bibliotecas que foram empregados para construir a sua proposta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crUBC3IQ3U_"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BluFtfHuCGzm"
      },
      "outputs": [],
      "source": [
        "#@title **Avaliação**\n",
        "GitHub = 10 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Implementacao_Model_Code = 7 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Aplicacao_Streamlit = 9 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Texto_Artigo  = 6 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Video = 7 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "Geral = 7 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "2Gqw7hUZHyle",
        "outputId": "cf56d67f-e7bc-42f0-a81d-f1f808967e93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nota final do trabalho 7.9\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"alunos\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"tia\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1115677\",\n          \"1115665\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nome\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \" DANIEL HENRIQUE\",\n          \" ADRIANA FUJITA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nota\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 7.9,\n        \"max\": 7.9,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "alunos"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b0360d6d-a298-4195-9914-febf9bafcd63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tia</th>\n",
              "      <th>nome</th>\n",
              "      <th>nota</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1115665</td>\n",
              "      <td>ADRIANA FUJITA</td>\n",
              "      <td>7.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1115677</td>\n",
              "      <td>DANIEL HENRIQUE</td>\n",
              "      <td>7.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0360d6d-a298-4195-9914-febf9bafcd63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0360d6d-a298-4195-9914-febf9bafcd63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0360d6d-a298-4195-9914-febf9bafcd63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-120aa0b0-48d8-4778-aca7-bace2443a5e8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-120aa0b0-48d8-4778-aca7-bace2443a5e8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-120aa0b0-48d8-4778-aca7-bace2443a5e8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c5110ba6-c42d-43ec-931e-d08b9033ac7e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('alunos')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c5110ba6-c42d-43ec-931e-d08b9033ac7e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('alunos');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       tia              nome  nota\n",
              "0  1115665    ADRIANA FUJITA   7.9\n",
              "1  1115677   DANIEL HENRIQUE   7.9"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title **Nota Final**\n",
        "\n",
        "nota = 2*GitHub + 4*Implementacao_Model_Code + 2*Aplicacao_Streamlit + 1*Texto_Artigo + 1*Video\n",
        "\n",
        "nota = nota / 10\n",
        "\n",
        "print(f'Nota final do trabalho {nota :.1f}')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "alunos = pd.DataFrame()\n",
        "\n",
        "lista_tia = []\n",
        "lista_nome = []\n",
        "\n",
        "for i in range(1,6):\n",
        "  exec(\"if Aluno\" + str(i) + \" !='None':  lista = Aluno\" + str(i) + \".split(','); lista_tia.append(lista[0]); lista_nome.append(lista[1].upper())\")\n",
        "\n",
        "alunos['tia'] = lista_tia\n",
        "alunos['nome'] = lista_nome\n",
        "alunos['nota'] = np.round(nota,1)\n",
        "print()\n",
        "display(alunos)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}